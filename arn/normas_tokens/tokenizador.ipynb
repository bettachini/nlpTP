{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-procesamiento del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de vectorizar (generar _embeddings_) con algún modelo, hay que _sanitizar_ el texto.\n",
    "\n",
    "ChatGPT instructed me to do this:\n",
    "\n",
    "- Tokenization: Break down your text into smaller units (tokens), like words or subwords.\n",
    "- Cleaning: Remove noise like punctuation, stopwords, or special characters, depending on the nature of your data.\n",
    "- Normalization: Convert everything to lowercase or perform stemming/lemmatization if necessary, though this can vary by use case.\n",
    "\n",
    "Libraries like SpaCy or NLTK can help with preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codespaces\n"
     ]
    }
   ],
   "source": [
    "# Determina entorno de ejecución: local o Colab\n",
    "\n",
    "# if (firstrun):\n",
    "if('google.colab' in str(get_ipython() ) ):\n",
    "    environment= 'colab'\n",
    "else:\n",
    "    import os\n",
    "    if (os.environ.get('PWD')=='/kaggle/working'):\n",
    "        environment= 'kaggle'\n",
    "    elif ( os.path.exists('/workspaces') ):\n",
    "        environment= 'codespaces'\n",
    "    else:\n",
    "        environment= 'local'\n",
    "print(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio base ( cambiar según el sistema de archivos de cada uno)\n",
    "current = 'nlpTP'\n",
    "\n",
    "# if (firstrun):\n",
    "if( environment== 'local' ):\n",
    "    system_path = '/home/vbettachini/documents/universitet/FCEyN/maestríaDatos/' + current + '/'\n",
    "elif( ( environment== 'colab' ) ):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    system_path = '/content/drive/MyDrive/maestría/' + current +'/'\n",
    "elif( ( environment== 'codespaces' ) ):\n",
    "    system_path = '/workspaces/' + current + '/'\n",
    "elif( ( environment== 'kaggle' ) )  :\n",
    "    a= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de texto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_path = system_path + 'arn/normas_tokens/'\n",
    "test_file = '4-2-3_r2.txt'\n",
    "file_path = work_path + test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text from file at file_path\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "texto = load_text(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza (cleansing ) | ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuente: https://dylancastillo.co/posts/nlp-snippets-clean-and-tokenize-text-with-python.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), text normalization and text cleansing are essential preprocessing operations aimed at preparing text data for analysis or modeling. While these terms sometimes overlap, they generally refer to different sets of tasks. Here's a breakdown of what each typically involves:\n",
    "Text Cleansing\n",
    "\n",
    "Text cleansing refers to operations that remove or correct unwanted elements in the text to make it cleaner and more consistent for processing. It usually involves:\n",
    "\n",
    "- Removing unnecessary characters: Deleting punctuation marks, special symbols, emojis, and other non-textual content (e.g., !, #, @, etc.).\n",
    "- Removing extra whitespace: Stripping unnecessary spaces, tabs, or newlines.\n",
    "- Correcting spelling or grammatical errors: Fixing typos or misspelled words, though this may be considered part of more advanced cleaning.\n",
    "- Filtering stopwords: Removing common words (like \"the,\" \"and,\" etc.) that do not contribute significant meaning.\n",
    "    - Implemented with NLTK\n",
    "- Removing URLs or email addresses: Cleaning the text of extraneous web addresses or emails that are not useful for text analysis.\n",
    "- Handling special tokens: Removing or replacing certain patterns like hashtags, mentions, or digits that don't add to the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quitar signos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s que puedan afectar la seguridad radiológica o nuclear\\n\\nB ALCANCE\\n2 Esta norma es aplicable al diseño puesta en marcha y operación de reactores de investigación\\nEl cumplimiento de la presente norma y'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinPuntuación = re.sub(f\"[{re.escape(punctuation)}]\", \"\", texto)\n",
    "sinPuntuación[500:700]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de texto (normalisation) | ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization involves transforming the text into a consistent, standardized format to make it suitable for algorithms that need regular input formats. It includes:\n",
    "\n",
    "- Lowercasing: Converting all text to lowercase to avoid treating words like \"Apple\" and \"apple\" differently.\n",
    "    - str.casefold()\n",
    "- Expanding contractions: Converting contractions like \"can't\" to \"cannot\" for consistency.\n",
    "- Stemming: Reducing words to their base or root form (e.g., \"running\" to \"run\") using heuristics.\n",
    "- Lemmatization: Converting words to their canonical base form, considering their part of speech (e.g., \"better\" to \"good\").\n",
    "- Standardizing formats: Ensuring dates, numbers, currencies, and other entities follow a consistent format.\n",
    "- Accented character normalization: Converting accented characters to their non-accented equivalents (e.g., \"é\" to \"e\").\n",
    "- Handling Unicode or ASCII conversions: Converting different encodings into a unified format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minúsculas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer letra en mayúscula -> minúscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minúsculas_texto = texto.lower()\n",
    "minúsculas_texto = texto.casefold() # solo la primer letra en minúscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'das de incendios, que puedan afectar la seguridad radiológica o nuclear.\\n\\nb. alcance\\n2. esta norma es aplicable al diseño, puesta en marcha y operación de reactores de investigación.\\nel cumplimiento d'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minúsculas_texto[500:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización | NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "  import nltk\n",
    "except:\n",
    "  !pip3 install nltk\n",
    "  import nltk\n",
    "# nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = minúsculas_texto\n",
    "tokens = word_tokenize(input, language=\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quitar _stopwords_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords_ = set(stopwords.words(\"spanish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autoridad',\n",
       " 'regulatoria',\n",
       " 'nuclear',\n",
       " 'dependiente',\n",
       " 'presidencia',\n",
       " 'nacion',\n",
       " 'ar',\n",
       " 'revisión',\n",
       " 'seguridad',\n",
       " 'incendios',\n",
       " 'reactores',\n",
       " 'investigación',\n",
       " 'aprobada',\n",
       " 'resolución',\n",
       " 'directorio',\n",
       " 'autoridad',\n",
       " 'regulatoria',\n",
       " 'nuclear',\n",
       " 'nº',\n",
       " 'boletín',\n",
       " 'oficial',\n",
       " 'república',\n",
       " 'argentina',\n",
       " 'norma',\n",
       " 'ar',\n",
       " 'seguridad',\n",
       " 'incendios',\n",
       " 'reactores',\n",
       " 'investigación',\n",
       " 'objetivo',\n",
       " 'establecer',\n",
       " 'criterios',\n",
       " 'seguridad',\n",
       " 'incendios',\n",
       " 'eventos',\n",
       " 'generados',\n",
       " 'explosiones',\n",
       " 'derivadas',\n",
       " 'incendios',\n",
       " 'puedan']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens = [token.lower() for token in tokens if token.lower() not in stopwords_ and token.isalpha()]\n",
    "# clean_tokens = [t for t in tokens if not t in stopwords_]\n",
    "clean_tokens[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autoridad',\n",
       " 'regulatoria',\n",
       " 'nuclear',\n",
       " 'dependient',\n",
       " 'presidencia',\n",
       " 'nacion',\n",
       " 'ar',\n",
       " 'revisión',\n",
       " 'seguridad',\n",
       " 'incendio',\n",
       " 'reactor',\n",
       " 'investigación',\n",
       " 'aprobada',\n",
       " 'resolución',\n",
       " 'directorio',\n",
       " 'autoridad',\n",
       " 'regulatoria',\n",
       " 'nuclear',\n",
       " 'nº',\n",
       " 'boletín',\n",
       " 'ofici',\n",
       " 'república',\n",
       " 'argentina',\n",
       " 'norma',\n",
       " 'ar',\n",
       " 'seguridad',\n",
       " 'incendio',\n",
       " 'reactor',\n",
       " 'investigación',\n",
       " 'objetivo',\n",
       " 'establec',\n",
       " 'criterio',\n",
       " 'seguridad',\n",
       " 'incendio',\n",
       " 'evento',\n",
       " 'generado',\n",
       " 'explosion',\n",
       " 'derivada',\n",
       " 'incendio',\n",
       " 'puedan']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = clean_tokens\n",
    "# stemming with a for loop\n",
    "stemmed_tokens =  [stemmer.stem(word) for word in clean_tokens]\n",
    "stemmed_tokens[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autoridad',\n",
       " 'regulatoria',\n",
       " 'nuclear',\n",
       " 'dependiente',\n",
       " 'presidencia',\n",
       " 'nacion',\n",
       " 'ar',\n",
       " 'revisión',\n",
       " 'seguridad',\n",
       " 'incendios',\n",
       " 'reactores',\n",
       " 'investigación',\n",
       " 'aprobada',\n",
       " 'resolución',\n",
       " 'directorio',\n",
       " 'autoridad',\n",
       " 'regulatoria',\n",
       " 'nuclear',\n",
       " 'nº',\n",
       " 'boletín',\n",
       " 'oficial',\n",
       " 'república',\n",
       " 'argentina',\n",
       " 'norma',\n",
       " 'ar',\n",
       " 'seguridad',\n",
       " 'incendios',\n",
       " 'reactores',\n",
       " 'investigación',\n",
       " 'objetivo',\n",
       " 'establecer',\n",
       " 'criterios',\n",
       " 'seguridad',\n",
       " 'incendios',\n",
       " 'eventos',\n",
       " 'generados',\n",
       " 'explosiones',\n",
       " 'derivadas',\n",
       " 'incendios',\n",
       " 'puedan']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = stemmed_tokens\n",
    "# stemming with a for loop\n",
    "lemmatized_tokens =  [lemmatizer.lemmatize(word) for word in clean_tokens]\n",
    "lemmatized_tokens[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpiar basura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "términos_basura = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
